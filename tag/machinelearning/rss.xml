<?xml version="1.0" encoding="UTF-8" ?>

<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
   
      <title>yjksw.github.io/</title>
   
   <link></link>
   <description>A beautiful narrative written with the world's most elegant publishing platform. The story begins here.</description>
   <language>en-uk</language>
   <managingEditor> </managingEditor>
   <atom:link href="rss" rel="self" type="application/rss+xml" />
   
	<item>
	  <title>[머신러닝] 딥러닝 영화 개인화 추천 - Part.3</title>
	  <link>//movie-dlrm-3</link>
	  <author></author>
	  <pubDate>2020-08-24T10:18:00+00:00</pubDate>
	  <guid>//movie-dlrm-3</guid>
	  <description><![CDATA[
	     <p>영화 개인화 추천 포스트의 마지막 파트이다. 이전까지는 데이터를 입력받아서 전처리하고, 모델을 구축하여 학습의 원리에 대해서 간략하게 알아보았다. 결국 학습을 하면서 loss 값을 optimizing 하는 과정에서 학습된 사용자의 영화에 대한 예측 평점인 y_hat이 출력되는 것인데, 검증셋에 대해서 출력된 y_hat의 값에 대해서 어느 정도의 후처리를 하여 사용자에게 추천할 영화 리스트를 선별하는 작업이 필요하다. 이번 포스트는 학습된 모델을 가지고 검증 및 테스트를 진행한 후에 모델에서 출력한 예측 평점에 대해서 사용자에게 추천할 영화 리스트를 추출하는 작업에 대해서 이야기해보자.</p>

<h2 id="개요">개요</h2>

<p><strong>전체적인 검증 및 테스트 과정은 다음과 같다.</strong></p>

<ol>
  <li>모델을 포팅해 사용자와 추천 영화 갯수를 입력한다.</li>
  <li>영화 목록이 들어가있는 입력 데이터를 모델에 넣어 prediction을 진행한다.</li>
  <li>모델에서 나온 예측값과 <strong>movieId</strong> 가 있는 데이터를 concat 하여 이어붙인다.</li>
  <li><em>movies.csv</em> 파일에 있는 영화 아이디별 영화 제목을 불러서 3번 데이터에 concat 한다.</li>
  <li>예측 평점을 기준으로 sorting 하여 내림차순 정렬을 한다.</li>
  <li>1번에서 입력된 영화 갯수만큼 잘라서 영화 제목을 출력해준다.</li>
</ol>

<h2 id="세부-구현">세부 구현</h2>

<p>파이썬으로 구현한 데이터 후처리 부분이다.</p>

<ol>
  <li>입력 데이터에 대한 인코딩 후 모델에 넣어 학습하기</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">target_usr</span> <span class="o">=</span> <span class="n">df_test</span><span class="p">[</span><span class="s">'userId'</span><span class="p">]</span> <span class="o">==</span> <span class="n">userId</span> <span class="c">#특정 유저에 대해서 filtering 하기 </span>
<span class="n">target_df</span> <span class="o">=</span> <span class="n">df_test</span><span class="p">[</span><span class="n">target_usr</span><span class="p">]</span>
<span class="n">users</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">target_df</span><span class="o">.</span><span class="n">userId</span><span class="o">.</span><span class="n">values</span><span class="p">)</span> <span class="c">#유저 목록 데이터 </span>
<span class="n">items</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">target_df</span><span class="o">.</span><span class="n">movieId</span><span class="o">.</span><span class="n">values</span><span class="p">)</span> <span class="c"># 영화 목록 데이터</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">users</span><span class="p">,</span> <span class="n">items</span><span class="p">)</span> <span class="c">#예측 ratings y_hat에 저장</span>
</code></pre></div></div>

<ol>
  <li>기존 영화 목록 데이터에 예측 평점 매칭하여 이어붙이기</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">target_df</span> <span class="o">=</span> <span class="n">target_df</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">new_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">target_df</span><span class="p">,</span> <span class="n">predictions</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c">#이어붙인 dataframe</span>
</code></pre></div></div>

<p>아래 이미지는 검증을 위해서 입력된 데이터를 모델에 넣어 학습했을 때 나온 결과 예측 rating이다.</p>

<p><img src="https://user-images.githubusercontent.com/63405904/110633039-a0341e80-81eb-11eb-97cf-9042280a7c5f.png" alt="image" /></p>

<ol>
  <li>영화 ID 별 영화 제목 매칭하기</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">movies</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'./ml-latest-small/movies.csv'</span><span class="p">)</span>
<span class="n">new_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">new_df</span><span class="p">,</span> <span class="n">movies</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="s">'movieId'</span><span class="p">)</span>
</code></pre></div></div>

<p>오른쪽에 나와있는 영화제목 데이터를 위의 dataframe에 이어붙여서 영화 아이디에 대한 영화 제목을 매칭하는 과정이 필요하다.</p>

<p><img src="https://user-images.githubusercontent.com/63405904/110632972-90b4d580-81eb-11eb-8f8b-ff8adc76abe5.png" alt="image" /></p>

<p>최종 영화 목록을 선별하여 출력하기 이전에 필요한 모든 dataframe들을 concat 했을때의 화면이다. 모든 데이터들을 이어붙였기 때문에 다소 지저분하지만 한눈에 볼 수 있는 장점이 있다.</p>

<p><img src="https://user-images.githubusercontent.com/63405904/110633113-b8a43900-81eb-11eb-9c31-15ac27ece25d.png" alt="image" /></p>

<h2 id="결과">결과</h2>

<p>다음과 같은 입력에 대한 추천 영화 목록이 아웃풋으로 나온다. 아래 결과는 사용자 1에 대한 상위 6개의 영화추천이다.</p>

<p><img src="https://user-images.githubusercontent.com/63405904/110633077-ad510d80-81eb-11eb-9a9e-e53a771b4201.png" alt="image" /></p>

<p>overfitting이나 underfitting이 되지 않고, 최적화된 학습 모델을 찾기 위해서는 여러번의 learning rate 조정 분석이 필요하다. 이것은 정답이 없기 때문에 충분한 데이터를 가지고 learning rate와 epoch를 변경하면서 여러 테스트 과정이 필요하다. 나도 굉장히 많은 learning rate를 시도해보면 실제 검증 validation loss와 train loss 를 비교하고 그래프를 그려가면서 나름 최적화 되었다고 생각하는 지점을 찾는 과정을 거쳤다.</p>

<p><img src="https://user-images.githubusercontent.com/63405904/110632879-7aa71500-81eb-11eb-9673-65d53a82f20e.png" alt="image" /></p>

<p>매우 간단한 구현이었지만 딥러닝에 대해서 더욱 알 수 있는 경험이었다. 무엇보다 우리 삶에 밀접하게 쓰여지고 있는 기술을 직접 구현해보고 처리해보니 더욱 흥미로웠다. 그렇지만 당근마켓, 페이스북, 유투브의 개인화 추천관련 논문과 블로그 등을 읽어보니, 정말 고려해야할 것도 필터링 해야 할 부분도 많다.</p>

<p><strong>생각해볼 것</strong>은 다음과 같다.</p>

<ol>
  <li>데이터베이스에 있는 모든 영화목록을 매번 넣어서 예측 평점을 모두 predict 한 뒤, sorting 하여 최상위 예측 평점 순으로 영화를 고르는 것은 매우 비효율 적이다. 데이터 베이스에 있는 영화가 100만개가 넘어가거나 할 경우 매번 그것을 반복할 수 없기 때문이다. 당근마켓의 블로그를 참고해보면 1차로 후보모델을 추출한 뒤에 일부 후보를 추리는 1차 작업을 한 뒤에, 해당 후보 모델에서 랭킹을 매기는 랭킹모델을 구축한다. 이렇듯 여러 과정을 거쳐서 일부 후보들을 filtering 하는 과정이 필요할 듯 하다.</li>
  <li>cold start 문제를 고려해야 한다. 처음 들어온 유저, 아무도 랭킹을 매기지 않은 영화에 대해서는 영영 추천하지 못하는 상황이 되어버릴 것이다. cold start에 대한 여러 고찰과 방법들에 대해서 나와있으니 한번 참고해보자.</li>
</ol>

	  ]]></description>
	</item>

	<item>
	  <title>[머신러닝] 딥러닝 영화 개인화 추천 - Part.2</title>
	  <link>//movie-dlrm-2</link>
	  <author></author>
	  <pubDate>2020-08-22T10:18:00+00:00</pubDate>
	  <guid>//movie-dlrm-2</guid>
	  <description><![CDATA[
	     <p>이어서 딥러닝 영화 개인화 추천 모델을 구현하면서 구축한 딥러닝 협업 필터링 모델 부분에 대한 코드를 살펴보면서 딥러닝 전체적인 흐름에 대해서 짚어보자. 앞에서 언급했듯이 코드는 다음 <a href="https://jyoondev.tistory.com/65?category=823946">링크</a>에서 참고하여 모델과 전체적인 데이터 처리를 진행했고, 이후에 학습된 output에 대한 데이터 및 결과 후처리는 추가 구현했다. 해당 부분은 다음 파트에 다루도록 하겠다.</p>

<h3 id="모델-전체-코드">모델 전체 코드</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">NNCollabFiltering</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_users</span><span class="p">,</span> <span class="n">num_items</span><span class="p">,</span> <span class="n">emb_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_hidden</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">NNCollabFiltering</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">user_emb</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">num_users</span><span class="p">,</span> <span class="n">emb_size</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">item_emb</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">num_items</span><span class="p">,</span> <span class="n">emb_size</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lin1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">emb_size</span><span class="o">*</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lin2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">drop1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
    
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
    <span class="n">U</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">user_emb</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>
    <span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">item_emb</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">U</span><span class="p">,</span> <span class="n">V</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lin1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lin2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div>

<h3 id="step1-임베딩---nnembedding">Step1. 임베딩 - nn.Embedding()</h3>

<p>위 코드를 보면 먼저 User와 Item에 관한 임베딩으로 시작한다. 파이토치에서 임베딩 벡터를 사용하는 방법은 크게 두가지가 있는데 그 중 위의 <code class="highlighter-rouge">nn.Embedding()</code> 은 embedding layer를 만들어서 훈련 데이터로부터 처음부터 임베딩 벡터를 학습하는 방법이다.</p>

<p>먼저 임베딩 층의 입력으로 사용하기 위해서는 정수 인코딩이 되어 있어야 한다. 즉 다음과 같은 단계를 따른다.</p>

<blockquote>
  <p>어떤 단어 -&gt; 고유한 정수로 인코딩 -&gt; 임베딩 층을 통과 -&gt; 밀집 벡터</p>
</blockquote>

<p>즉, 고유한 정수 인코딩 값에 대해서 밀집 벡터(dense vector)를 맵핑해주는 것이다. 이 밀집 베터가 흔히 알고 있는 임베딩 벡터이다. 임베딩을 시킨다는 것은 어떠한 단어에 대한 고유 인코딩 정수값을 인덱스로 가지고 있는 룩업 테이블에서 해당 임베딩 벡터 값을 가져오는 것이다. 또한 <mark>이 테이블은 단어 집합만큼의 행을 가지고 있으므로 모든 단어들은 고유한 임베딩 벡터를 보유하게 된다.</mark> 이러한 벡터 값을 담고 있는 룩업 테이블을 생성하는 것이 <code class="highlighter-rouge">nn.Embedding()</code>의 역할이다.</p>

<p><img src="https://user-images.githubusercontent.com/63405904/110482824-6bad5d80-812c-11eb-9da2-750e1946c80b.png" alt="image" /></p>

<p>위의 그림을 참고해보면 단어 ‘great’에 대한 임베딩 벡터가 4차원인 것을 확인할 수 있다. 해당 차원값은 parameter로 넘겨줄 수 있는 부분이다. 이렇게 생성된 임베딩 벡터는 모델의 입력이 되고, 역전파 과정을 거치면서 바로 이 임베딩 벡터값이 학습 되는 것이다.</p>

<h5 id="-코드에서-임베딩">&gt; 코드에서 임베딩</h5>

<p>위 코드에서 임베딩이 어떻게 이루어지고 있는지 살펴보자. 코드를 살펴보면 임베딩 관련한 부분에 다음과 같이 있다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#def __init__ 메소드 내: </span>
  <span class="bp">self</span><span class="o">.</span><span class="n">user_emb</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">num_users</span><span class="p">,</span> <span class="n">emb_size</span><span class="p">)</span>
  <span class="bp">self</span><span class="o">.</span><span class="n">item_emb</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">num_items</span><span class="p">,</span> <span class="n">emb_size</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="highlighter-rouge">nn.Embedding()</code> 에 넘겨지는 parameter는 크게 <strong>2가지</strong>가 있다. 1) 테이블 사이즈 (단어 및 데이터 갯수) 2) 임베딩 사이즈 (embedding vector  차원).</p>

<p>영화에서 모델에 넣어서 학습할 데이터는 사용자 user와 영화 item이다. 이 두개에 대한 임베딩 테이블을 생성하기 위해서 인코딩 하며 중복없이 뽑아낸 user 와 item 리스트의 크기와 임베딩 사이즈를 결정해서 <code class="highlighter-rouge">nn.Embbeding()</code>을 호출한다. 그럼 임베딩 테이블이 생성되어 각각 user_emb 와 item_emb에 저장된다.</p>

<h3 id="step2-linear-layer-생성---nnlinear">Step2. Linear Layer 생성 - nn.Linear()</h3>

<p>다음 코드에서는 Linear layer를 생성한다. 딥러닝의 핵심인 신경망(neural network) 층을 쌓아올려서 학습을 진행한다. 그때 필요한 신경망 층을 생성하는 역할을 한다. 딥러닝을 위한 신경망은 기본적으로 선형회귀분석을 기본으로 하기 때문에 선형변형 함수로 층을 쌓는다.</p>

<p><img src="https://user-images.githubusercontent.com/63405904/110482886-7d8f0080-812c-11eb-9b12-6c65a734be69.png" alt="image" /></p>

<p>파이토치에서 제공하는 <a href="https://pytorch.org/docs/stable/nn.html#linear-layers">document</a>를 살펴보면 위와 같은 선형변형 함수를 사용하는 것을 확인할 수 있다. 선형결합은 보존하는 선형변형 함수를 생성하고 원하는 <strong>in_feature</strong>와 <strong>out_feature</strong>의 사이즈를 parameter로 넘긴다.</p>

<h5 id="-코드에서-layer-생성">&gt; 코드에서 layer 생성</h5>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#def __init__ 메소드 내:</span>
	<span class="bp">self</span><span class="o">.</span><span class="n">lin1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">emb_size</span><span class="o">*</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">)</span>
  <span class="bp">self</span><span class="o">.</span><span class="n">lin2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>위 코드는 입력 차원이 emb_size의 두배인 input sample에 대해서 n_hidden 사이즈 만큼의 차원으로 선형변형을 하는 linear layer 하나와, n_hidden 사이즈의 input sample에 대해서 1로 선형변형을 하는 linear layer 두개를 생성한다.</p>

<h3 id="step3-모델-일반화---nndropout">Step3. 모델 일반화 - nn.Dropout()</h3>

<p>Dropout은 모델을 일반화 기법으로 일부 파라미터를 학습에 반영하지 않는 것이다. Validation과 test 시에는 적용하지 않고 train 시에 dropout을 적용하는데, 일종의 정규화 기법이라고 볼 수 있다. 모델을 학습할 때 과적합(overfitting)의 위험을 줄이고, 학습속도를 개선하는 문제를 해결하기 위한 방법이다. 모델을 학습할 때 지나치게 학습 데이터에 대한 높은 정확도를 보이기 보다, 범용적으로 사용될 수 있도록 overfitting 문제를 피하기 위해서 고안된 해결책 중 하나이다. 일반적으로 신경망의 층이 깊어지고, 학습률이 작을수록 overfitting이 될 가능성이 높다.</p>

<p>이중 본 코드에서 사용하고 있는 모델 일반화의 방법은 드롭아웃 Dropout이다. 신경망 모델이 지나치게 복잡해질 때, 뉴런의 연결을 임의로 삭제하여 전달하지 않도록 떨어뜨리는 역할을 한다. 다만, 테스트를 할 때에는 모든 뉴런을 사용하기 때문에 반드시 학습시에만 드롭아웃을 적용해야 한다.</p>

<p><img src="https://user-images.githubusercontent.com/63405904/110482706-47ea1780-812c-11eb-9908-5686512aae8c.png" alt="image" /></p>

<p>파이토치 <a href="https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout">document</a>를 보면 <code class="highlighter-rouge">nn.torch</code> 모듈에서 드롭아웃 또한 지원을 한다.</p>

<p><img src="https://user-images.githubusercontent.com/63405904/110482789-5fc19b80-812c-11eb-960c-8f7da3b2597f.png" alt="image" /></p>

<p>파이토치에 제공하는 도큐멘트를 살펴보면 학습시 무작위로 몇개의 뉴런들에 대해서 <em>p</em> 확률만큼  ‘zeros’ 시킨다고 나와있다. 이때 <em>p</em>는 parameter로 주어지는 확률 변수이고, default는 0.5이다. <code class="highlighter-rouge">forward</code>함수가 호출될 때마다 적용되도록 되어 있다.</p>

<h5 id="-코드에서-dropout">&gt; 코드에서 Dropout</h5>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#def __init__ 메소드 내:</span>
	<span class="bp">self</span><span class="o">.</span><span class="n">drop1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
</code></pre></div></div>

<p>본 코드를 살펴보면 nn.torch 모둘에서 Dropout 함수를 호출하고 확률 변수를 0.1로 주었다.</p>

<h3 id="step4-활성화함수---torchnnfunctionalrelu">Step4. 활성화함수 - torch.nn.functional.relu()</h3>

<p>다음 딥러닝 신경망 모델 구축에서 중요한 부분은 활성화 함수(Activation Function)이다. 활성화 함수는 최종출력 신호 후, 다음 뉴런으로 보낼지 말지를 결정하는 함수이다. 즉, 특정 뉴런이 다음 뉴런으로 신호를 보낼 때 입력신호의 어떠한 기준에 따라서 보내고 보내지 않는지를 결정하도록 하는 것이다. 딥러닝에서는 뉴런들을 다음 레이어로 전달할 때 비선형 함수를 통화시킨 후 전달하도록 하는데 이때 사용되는 함수가 활성화 함수이다.</p>

<p>딥러닝 학습의 핵심은 이름에서 볼 수 있듯이 깊게 층을 쌓아서 그 층을 통과하면서 학습되는 것인데, 선형함수를 사용하게 되면 층을 깊게 하는 의미가 줄어들게 된다. 해당 설명은 [밑바닥부터 시작하는 딥러닝] 책의 한 부분을 인용하도록 하겠다.</p>

<blockquote>
  <p>선형합수인 h(x)=cx를 활성화함수로 사용한 3층 네트워크를 떠올려 보세요. 이를 식으로 나타내면 y(x) = h(h(h(x)))가 됩니다. 이는 실은 y(x)=ax와 똑같은 식 입니다. a=c3이라고 하면 끝이죠. 즉, 은닉층이 없는 네트워크로 표현할 수 있습니다. 뉴럴네트워크에서 층을 쌓는 혜택을 얻고 싶다면 활성화 함수로는 반드시 비선형 함수를 사용해야 한다.</p>

  <p>-밑바닥부터 시작하는 딥러닝-</p>
</blockquote>

<p>이러한 역할을 하는 활성화 함수는 많은 종류가 있다. <strong>1) 시그모이드 함수 2) tanh 함수 3) ReLU 함수.</strong>  본 코드에서는 가장 많이 사용되는 활성화 함수인 ReLU 함수를 사용했다.</p>

<p><img src="https://user-images.githubusercontent.com/63405904/110482937-8b448600-812c-11eb-9672-04dc7c7e5928.png" alt="image" /></p>

<p>ReLU 함수를 살펴보면 $x &gt; 0$ 이면 기울기가 1인 직선이고 $x &lt; 0$이면 함수값이 0이 된다. 따라서 다른 활성화함수에 비해서 굉장히 간단하고 빠르다. 해당 함수는 양수에서는 Linear function 과 같은 모습을 보이지만 음수의 경우 0으로 버려지므로 non-linear 한 함수로 작동하여 layer를 깊게 쌓을 수 있는 장점을 가진다.</p>

<h5 id="-코드에서-활성화함수">&gt; 코드에서 활성화함수</h5>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#def forward 내</span>
	<span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">U</span><span class="p">,</span> <span class="n">V</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
</code></pre></div></div>

<p>선형함수가 linear layer에 들어가기 전에 비선형 함수를 거친다. 위의 코드에서 F는 <code class="highlighter-rouge">torch.nn.funtional</code>모듈이며 모듈 내에 있는 <code class="highlighter-rouge">relu()</code>를 사용하고 있다.</p>

<p><strong><small> [참고 자료]: https://wikidocs.net/64779, https://tutorials.pytorch.kr/beginner/blitz/neural_networks_tutorial.html, https://pytorch.org/docs/, https://yeomko.tistory.com/39, https://reniew.github.io/12/, https://eda-ai-lab.tistory.com/405, https://jyoondev.tistory.com/65?category=823946, https://sacko.tistory.com/45</small></strong></p>


	  ]]></description>
	</item>

	<item>
	  <title>[머신러닝] 딥러닝 영화 개인화 추천 - Part.1</title>
	  <link>//movie-dlrm-1</link>
	  <author></author>
	  <pubDate>2020-08-21T10:18:00+00:00</pubDate>
	  <guid>//movie-dlrm-1</guid>
	  <description><![CDATA[
	     <p>인턴을 하는 중에 요즘에 중요한 머신러닝의 한 분야가 되고 있는 개인화 추천에 대한 개발을 맡게 되었다. 요즘 넷플릭스, 왓챠와 같은 OTT 서비스는 물론이고, SNS에 표기되는 광고, 당근마켓 등등과 같은 중고거래 및 쇼핑 어플리케이션에서도 중요한 것이 사용자의 취향을 분석하여서 알맞은 아이템을 추천하는 기술이 핵심이다. 어쩌면 사용자가 의식적으로 파악하고 있는 이상의 취향을 파악해서 추천해야 할 때도 있다. 이전 포스트에서 다루었듯이 개인화 추천에는 여러 통계기반 머신러닝 기법들이 있다. 그리고 인공 신경망이라는 딥러닝 기법이 등장하게 되면서 더욱 세밀하고 정확한 개인화 추천이 가능해졌다.</p>

<p>처음 접했기 때문에 매우 생소하고 낯선 분야였지만 많은 자료들을 찾아보면서 현재 내 삶(<small>유튜브나 넷플릭스의 노예…</small>)과 아주 밀접하게 연관이 되어 있는 많은 어플리케이션과 서비스등에 실제로 사용되고 있는 인공지능 기법이라는 것이 금방 흥미를 불러 일으켰다. 조금 어렵긴 하지만 facebook에서 공개한 DLRM(Deep Learning Recommendation Model) <a href="https://ai.facebook.com/blog/dlrm-an-advanced-open-source-deep-learning-recommendation-model/">자료</a>나 구글에서 공개한 유튜브 개인화 추천 <a href="https://research.google/pubs/pub45530/">논문</a>, 국내의 당근마켓에서 쓴 개인화 추천 관련 블로그 <a href="[https://medium.com/daangn/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EC%9D%B8%ED%99%94-%EC%B6%94%EC%B2%9C-1eda682c2e8c](https://medium.com/daangn/딥러닝-개인화-추천-1eda682c2e8c)">포스트</a> 등을 참고하면서 개인화 추천에 대한 대략적인 아이디어나 개념을 잡았다. <small>물론 매우 매우 어렵기 때문에 모든 것을 이해하는 것은 (나는) 힘들다. 가볍지만 꼼꼼하게 읽으면서 대략적인 맥락을 파악하는 것을 추천한다!</small></p>

<p>이후에 구현되어 있었던 pytorch를 활용한 딥러닝 모델 예시를 보게 되었고 참고하여 응용하면서 아주 간단한 딥러닝 영화 개인화 추천 모델을 구현하였다. 이번 포스트는 해당 개발의 초반, 데이터 전처리에 관한 내용을 다룰 예정이다. 전체 코드를 보고 싶다면 다음 <a href="https://github.com/yjksw/DeepLearning_Movie_Recommendation_System">github repo</a>를 참고하시길.</p>

<h3 id="데이터-모양새">데이터 모양새</h3>

<p>딥러닝 개인화 추천 모델 구현을 시작하면서 가장 먼저 한 일은 입력 데이터의 형식에 대해서 파악한 것이다. 본 모델을 사용자와 영화 아이템, 그리고 해당  영화에 대한 각 사용자의 ratings 데이터를 담고 있는 MovieLens 데이터를 사용하였다. 해당 사이트에 가서 원하는 파일을 다운받아서 열면 다음과 같은 형식으로 데이터가 담겨져 있다.</p>

<p><img src="https://user-images.githubusercontent.com/63405904/110306953-c027de80-8041-11eb-838e-1ca3bf9957f2.png" alt="image" /></p>

<p>해당 csv. 파일을 읽어서 validation 과 test 데이터를 나누어야 한다. 다음 명령어를 써서 읽은 csv 파일 data frame에 대한 마스크를 씌울 수 있다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">msk</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">))</span> <span class="o">&lt;</span> <span class="mf">0.8</span>
</code></pre></div></div>

<p>나는 구현할 때 80% 정도의 데이터를 학습 데이터로, 나머지 20%의 데이터를 검증 데이터로 사용했다.</p>

<h3 id="데이터-전처리">데이터 전처리</h3>

<p>데이터 전처리에서 해야할 일들은 매우 간단하다. 기본적으로 학습 데이터와 검증 데이터는 모두 비슷한 과정의 데이터 전처리 과정을 거치지만 개인화 추천을 위한 영화 데이터에 대해서 구현할 때는 검증 데이터 전처리에 관해 조금 다른 부분이 필요하기도 하다.</p>

<p>학습에서 사용되는 데이터는 ratings.csv 파일에서도 특히 <em>userId</em>와 <em>movieId</em> 부분이다. 따라서 해당 column를 하나씩 끌어다가 전처리를 해주어야 한다. 여기서 필요한 전처리 과정은 다음과 같다.</p>

<ol>
  <li>중복을 제거하여 순수한 유저와 아이템 리스트 갯수 및 리스트 생성</li>
  <li>학습 데이터가 아닌 검증 또는 테스트 데이터일 경우, 학습 데이터에 존재하지 않는 유저와 아이템에 대하여 -1 처리하여 제외</li>
</ol>

<p><strong>1번</strong> 과정이 필요한 이유는 이후에 신경망 layers를 쌓고 임베딩 할 때에 pytorch의 모듈을 사용하여서 임베딩 테이블을 생성해야하기 때문이다. <strong>2번</strong> 과정이 필요한 이유는 검증 또는 테스트 데이터에 학습 데이터로 학습되지 않은 전혀 새로운 유저나 영화가 나왔을 경우, 제대로 예측할 수 없기 때문에 미리 제거해 주는 것이다.</p>

<h3 id="코드">코드</h3>

<p>위의 데이터 전처리를 하기 위해서 유용한 몇가지 파이썬 코드를 소개한다.</p>

<ol>
  <li>데이터 columns의 중복제거</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">uni</span> <span class="o">=</span> <span class="n">train_col</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span>
</code></pre></div></div>

<p>위 메소드는 입력으로 들어온 column에 대해서 중복을 제거한 리스트를 uni 변수에 저장해준다.</p>

<ol>
  <li>존재하지 않는 아이템에 대하여 -1 처리하기</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">name2idx</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">col</span><span class="p">])</span>
</code></pre></div></div>

<p>여기서 name2idx는 중복이 없는 순수한 유저 또는 아이템 리스트와 해당 index이다. 위와 같은 코드를 실행하면 name2idx에서 해당 col에 있는 x 값이 있다면 해당 값을 가져오고, 그렇지 않다면 -1을 입력하도록 하는 간편한 파이썬 문법이다.</p>

<p>이후에 학습 데이터에 존재하지 않는 유저 또는 아이템이 음수로 들어와 있으니 해당 처리를 다음과 같이하면 제외하고 검증 및 테스트 할 수 있다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="n">col_name</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">]</span>
</code></pre></div></div>

<h3 id="결과-화면">결과 화면</h3>

<p>위와 같은 데이터 전처리를 끝내면 다음과 같은 전처리된 데이터가 생성된다. 물론 임베딩하는 전처리가 추가로 필요하지만 다음 포스트에서 학습과 함께 다루도록 하겠다. 결과 화면의 데이터는 csv의 데이터와 크게 차이가 없이 지저분한 데이터들을 제외하고 처리해 놓은 데이터라고 생각하면 된다.</p>

<p><img src="https://user-images.githubusercontent.com/63405904/110306884-b00fff00-8041-11eb-91f0-51660920fbcf.png" alt="image" /></p>

<p><strong><small> [참고 자료]: https://jyoondev.tistory.com/65 </small></strong></p>


	  ]]></description>
	</item>

	<item>
	  <title>[머신러닝] 추천 시스템 기술</title>
	  <link>//ml-recommendation-system</link>
	  <author></author>
	  <pubDate>2020-08-10T10:18:00+00:00</pubDate>
	  <guid>//ml-recommendation-system</guid>
	  <description><![CDATA[
	     <p>모 기업에서 인턴을 하면서 맡은 업무가 <mark>개인화 추천 모델 구현</mark>이었다. 맡은 업무는 딥러닝 기반의 개인화 추천 모델을 제작하는 것이지만 기존에 회사에서 가지고 있는 추천 시스템의 경우 협업 필터링 등으로 이미 구현이 되어 있었기 때문에 간단히 개인화 추천 시스템에 대한 브리핑을 해주시면서 감을 잡을 수 있도록 해주셨다.</p>

<p>추천 시스템 기술을 처음 접해보면서 어떠한 것인지 공부하며 기록해보려고 한다.</p>

<h3 id="추천이란">추천이란?</h3>

<p>추천이란 간단히 말해서 사용자(user)에게 관심이 있을 것으로 예상이 되는 아이템(item)을 제안하는 것이다. 특정 아이템에 대한 특정 사용자의 선호도 또는 평가를 예측하는 것이 매우 중요하다.</p>

<p>우리가 흔히 생각해 낼 수 있는 추천 시스템은 페이스북과 같은 것에서의 광고 추천, 넷플릭스나 왓챠와 같은 OTT 서비스에서의 영화 추천 시스템이다.</p>

<h3 id="접근-방식">접근 방식</h3>

<p>추천 시스템 기술에서의 접근 방식은 크게 다음과 같은 3가지가 있다.</p>

<ul>
  <li>내용 기반 필터링 (Content-based Filtering)</li>
  <li>협업 필터링 (Collaborative Filtering)</li>
  <li>하이브리드 (Hybrid)</li>
</ul>

<h4 id="1-내용-기반-필터링">1) 내용 기반 필터링</h4>

<p>사용자의 프로필이나 아이템의 content 정보를 이용하는 방법이다. 사용자의 선호도, 취향 등을 파악하는 방법이 핵심이다. 예를 들어, 회원가입 시 사용자에게 선호하는 아이템 또는 분야 등에 대해서 선택하도록 하여 선호도를 파악하여 해당 정보를 기반으로 추천을 한다. 또는 사용자의 과거에 평가한 아이템 분석을 통해서 선호도를 파악할 수 있다.</p>

<p><img src="https://user-images.githubusercontent.com/63405904/110242569-0590d100-7f9a-11eb-8799-e9fe186779e6.jpeg" alt="contentbased" /></p>

<h4 id="2-협업-필터링">2) 협업 필터링</h4>

<p>협업 필터링은 여러 사용자들의 활동, 기호 정보들을 분석하여, 각 사아요자에게 적합한 아이템을 추천하도록 한다. 예를 들어서 사용자 A와 유사하다고 판단되는 사용자 B가 최근 구매한 상품을 사용자 A에게도 추천하도록 하는 것이다.</p>

<p><img src="https://user-images.githubusercontent.com/63405904/110242548-f14cd400-7f99-11eb-927e-2e1000c04eba.jpeg" alt="collaborative" /></p>

<h4 id="3-내용-기반-필터링--협업-필터링의-장단점">3) 내용 기반 필터링 &amp; 협업 필터링의 장단점</h4>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>내용 기반 필터링</th>
      <th>협업 필터링</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>장점</td>
      <td>사용자의 명시적인 기호 정보를 직접적으로 반영한다. <br />다른 사용자의 정보나 평가, 행동 등이 필요하지 않다.<br />새로 추가된 아이템에 대한 추천이 가능하다.</td>
      <td>대부분의 경우 추천 성과가 우수하다.<br />잠재적인 특징을 고려하여 다양한 범위의 추천이 가능하다.</td>
    </tr>
    <tr>
      <td>단점</td>
      <td>사용자의 명시적인 프로필 얻기 어려움이 있다.<br />명시적으로 표현된 특징만 다룰 수 있고 잠재적인 것을 캐치하기 어렵다.<br />추천하는 항목이 비슷한 장르에 머무르는 한계가 있다.</td>
      <td>초기 사용자에 대한 믿을만한 추천이 어렵다. Cold start가 존재한다.<br />한번도 평가되지 않은 아이템은 추천 대상에서 제외된다. sparsity, coverage</td>
    </tr>
  </tbody>
</table>

<h4 id="4-하이브리드">4) 하이브리드</h4>

<p>위에서의 내용 기반 필터링과 협업 필터링을 결합하여 사용한다. 결합하는 방식을 다양하기 때문에 어떠한 하이브리드 방식을 택하는지는 매우 광범위 하다. 하지만 간단히 보아서 위의 두가지 방식을 같이 사용함으로 각자의 단점을 보완한다는 장점을 가지고 있다. 다만 시스템적으로 매우 복잡해질 수 있는 단점이 있다.</p>

<h4 id="5-협업-필터링-기술-분류">5) 협업 필터링 기술 분류</h4>

<p>본 기업에서 중점적으로 사용하고 있는 협업 필터링 기술은 다음과 같이 분류된다.</p>

<ul>
  <li>메모리 기반(Memory-based) 협업 필터링
    <ul>
      <li>사용자 또는 아이템 간의 유사도를 계산하고 그것을 바탕을 추천 결과를 생성하는 방식으로 유사도를 계산하는 방식이 매우 중요함.</li>
      <li><strong>대표 알고리즘</strong>: User-based CF / Item-based CF</li>
      <li><strong>장점</strong>: 구현이 간단하고 이해하기 쉬움.</li>
      <li><strong>한계</strong>: 1) 새로운 사용자와 아이템에 대한 cold start 문제 2) Rating matrix의 sparsity 문제 3) 큰 데이터 셋에 대해 제한된 scalability</li>
    </ul>
  </li>
  <li>모델 기반(Model-based) 협업 필터링
    <ul>
      <li>데이터(rating matrix)에 내재되어 있는 패턴이나 속성을 학습한 모델을 만들고, 이것을 바탕으로 추천 결과를 생성하는 방식.</li>
      <li><strong>대표 알고리즘</strong>: Slope-One EF / Matrix Facotrization</li>
      <li><strong>장점:</strong> sparsity, scalability 문제에 상대적으로 더 잘 대처하는 것이 가능하고 예측 성능이 향상됨.</li>
      <li><strong>한계:</strong> 1) 모델 구축 비용이 큼 2) 예측 성능과 scalability 사이의 trade-off 3) 차원 감소로 인한 정보손실(SVD 실행 시 발생)</li>
    </ul>
  </li>
  <li>하이브리드(Hybrid) 방식 협업 필터링
    <ul>
      <li>메모리 기반 방식과 모델 기반 방식을 결합하여 사용하는 방식</li>
      <li><strong>대표 알고리즘</strong>: 메모리 기반과 모델 기반의 조합</li>
      <li><strong>장점</strong>: 각 방식의 단점을 보완하고 장점만을 취합할 수 있음</li>
      <li><strong>한계:</strong> 구현이 복잡해지고 비용이 증가함.</li>
    </ul>
  </li>
</ul>

<p><strong><small>[참고 자료]: https://www.samsungsemiconstory.com/2265, </small></strong></p>


	  ]]></description>
	</item>

	<item>
	  <title>[머신러닝]나이브 베이즈 분류기(Naive Bayes Classifier)</title>
	  <link>//naivebayes</link>
	  <author></author>
	  <pubDate>2020-07-24T10:18:00+00:00</pubDate>
	  <guid>//naivebayes</guid>
	  <description><![CDATA[
	     <p>강남의 어느 검색 솔루션 기업에서 인턴한지 어연 4주차가 지나간다. 중간 지점을 지나가면서 한 것을 정리할 겸 나이브베이즈 문서 분류기 구현과 이론에 대해서 정리해 보려고 한다. <small>최대한 쉽게!!</small></p>

<p>나이브 베이즈 분류기는 베이즈 정리(Bayes’ theorem)을 사용한 분류 알고리즘이다. 이것은 전통적으로 텍스트 분류를 하는 분류기로 인공지능의 기능을 기학적으로 올려준 인공 신경망 알고리즘은 아니지만 머신 러닝의 중요한 알고리즘 중 하나로 꽤 좋은 성능을 보인다. 나이브 베이즈 분류기에서 사용하는 베이즈 정리는 무엇일까?</p>

<h3 id="베이즈의-정리bayes-theorem를-사용한-분류-기법">베이즈의 정리(Bayes’ theorem)를 사용한 분류 기법</h3>

<p>베이즈 정리는 조건부 확률을 계산하는 방법 중 하나이다. 다음과 같이 표현할 수 있다.</p>

<ul>
  <li><strong><em>P(A)</em></strong>: 사전확률(Prior). 사건 B가 발생하기 전 A가 가지고 있던 확률</li>
  <li><strong><em>P(B)</em></strong>: 정규화 상수(normalizing constant). B가 일어날 확률</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>***P(B</td>
          <td>A)***: 가능도(likelihood). A가 발생한 경우 B가 일어날 확률</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>***P(A</td>
          <td>B)***: 사후확률(Posterior). B가 발생한 후 A가 일어날 확률</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<table>
  <tbody>
    <tr>
      <td>이 때 ***P(A</td>
      <td>B)***를 구하려면, 다음과 같은 수식을 사용한다.</td>
    </tr>
  </tbody>
</table>

<p><script type="math/tex">P(A|B) = \frac {P(B|A)P(A)} {P(B)}</script>
주로 나이브 베이즈 분류기법을 설명할 때 스팸 메일 분류기를 예를 들어서 설명한다.</p>

<h3 id="나이브-베이즈를-활용한-스팸-분류기">나이브 베이즈를 활용한 스팸 분류기</h3>

<p>어떤 문서 D에 대하여 해당 문서가 스팸(S)클래스에 속하는지 일반(!S)클래스에 속하는지 분류할 때 나이브 베이즈 분류 알고리즘을 사용한다고 하자. 그리고 이미 각기 다른 단어들에 대해서 해당 단어가 스팸일 확률과 일반일 확률에 대한 데이터가 이미 확보되어 있다고 가정한다. 한번 기호로 살펴보자. 우리가 가지고 있는 데이터는 다음 두개와 같다. 다음은 각각 “Sale이라는 단어는 60%의 확률로 스팸메일에서 발견되고, 30%의 확률로 일반메일에 발견된다.”라는 정보를 가지고 있는 것이다. 
<script type="math/tex">P('Sale'|S), P('Sale'|!S)</script>
 데이터를 수 만개의 단어들에 대한 위의 데이터를 활용해서 결국 풀고 싶은 문제는 $P(S|D)$와 $P(!S|D)$이다. 다음 두 확률을 구한 다음 확률이 더 큰 클래스에 해당 문서가 속한다고 결론을 내린다. $P(S|D)$는 문서 $D$가 주어졌다는 가정하에 해당 문서가 스팸일 조건부 확률을 나타낸다. 반대로 $P(!S|D)$는 문서 $D$가 주어졌다는 가정하에 해당 문서가 스팸이 아닐 조건부 확률을 나타낸다. 해당 문서를 어떤 클래스에 속하는지 분류하려면 먼저 문서에서 feature를 추출해야 한다. 추출된 feature들이 어떤 규칙에 의한 키워드 단어들이라고 할 때, 베이즈 정리는 특징벡터$x=(x1, x2, …, xn)$의 요소들이 모두 <strong>조건부 독립</strong>이라는 가정을 한다. 즉, 각 단어들이 서로 미치는 확률에 있어서 연관이 없다고 가정하는 것이다. <small>(이 부분에서 ‘Naive(순진한) ‘라는 이름이 붙는다. 실제로는 모두 독립적이지 않고 동등하지 않은데 이렇게 간주해버리는 순진함을 가지고 있다.)</small> 이때 해당 특징벡터 $x$에 대한 클래스 $S$에 속할 확률은 다음과 같다.
<script type="math/tex">P(S|x1,x2,...,xn)=\frac {P(x1,x2,...,xn)P(S)}{P(x1,x2,...,xn)}</script>
 앞에서 언급했듯이 각 feature의 요소들은 조건부 독립이기 때문에 다음과 같이 바꿔 쓸 수 있다. 
<script type="math/tex">P(S|x1,x2,...,xn)=\frac {P(x1|S)P(x2|S)...P(xn|S)P(S)}{P(x1)P(x2)...P(xn)}</script>
위와 같이 수식을 사용하면 문서를 특정 클래스들로 분류하기 위해서는 다음만 알면 된다. <strong>1) 문서로부터 특징벡터를 추출하는 방법 2)기존에 확보된 데이터로부터 $P(S|x)$와 $P(!S|x)$를 계산하는 방법 3)각 클래스의 비율인 사전확률 $P(S),P(!S)$.</strong> 해당 문제를 해결하기 위해 주어진 문서로 부터 판단에 사용할 특징feature를 추출해야 하는데 이때 어떤 확률분포를 사용하는지에 따라 특징벡터가 달라지게 된다.</p>

<h3 id="나이브-베이즈-분류기-3종류">나이브 베이즈 분류기 3종류</h3>

<p>나이브 베이즈 분류기에는 다음과 같이 3 종류가 있다. 세가지를 모두 다루지는 않고 인턴 기간동안 구현한 Bernoulli naive bayes classifier에 대해서 주로 다룰 것이다. 다만 3종류는 어떠한 것이 있고 각각의 특징과 다른점들에 대해서 간단히 설명하고 넘어가보자.</p>

<ol>
  <li>Gaussian naive bayes classifier: 설명변수가 연속형인 경우
    <ul>
      <li>연속적인 데이터에 적용 가능</li>
    </ul>
  </li>
  <li>Multinomial naive bayes classifier: 설명변수가 범주형인 경우
    <ul>
      <li>카운트 데이터(횟수)에 적용 가능</li>
    </ul>
  </li>
  <li>Bernoulli naive bayes classifier: 설명변수가 이분형인 경우
    <ul>
      <li>이진 데이터에 적용 가능.</li>
    </ul>
  </li>
</ol>

<p>Gaussian naive bayes는 주로 매우 고차원적인 데이터 세트를 다룰 때 사용된다. 나머지 두 베이즈 모델인 다항분포(Multinomial)과 베르누이(Bernoulli)는 보다 텍스트와 같은 데이터에 사용된다. 인턴하는 회사에서 요구한 업무는 문서에 대한 분류기이니 후자가 더 적합하다. 그 중, 내가 맡은 업무는 베르누이를 사용한 나이브 베이즈 문서 분류기이다.</p>

<h3 id="bernoulli-naive-bayes-분류기">Bernoulli Naive Bayes 분류기</h3>

<p>일부 코드를 제시하면서 구현 로직 설명을 하겠지만 인턴 회사의 코드이므로 모든 공개가 어렵다. 또한 여기서 참고할 점은 구현 언어가 회사 내에서 개발한 새로운 언어라는 것이다. 머신러닝을 사용한 인공지능 회사인 만큼 자체적으로 로직을 짜고 데이터를 처리하기에 더 적합한 언어를 포팅하여 사용하고 있다. 따라서 큰 로직만 참고하는 것을 추천한다.</p>

<p>먼저 텍스트 분류에 크게 사용되는 베르누이 확률 분포 모형과 다항분포 모형을 비교하며 간단히 어떤 차이가 있는지 살펴보자.</p>

<p>먼저 다항분포는 표본벡터 $x$가 있다고 가정했을 때, 이것을 $D$면을 가진 주사위를 $y$번 던진 결과라고 본다. 즉, $x=[1, 4, 0, 5]$가 있을 때, 다음 표본벡터는 4면체 주사위를 10번 던져서 1인 면이 1번, 2인 면이 4번, 4인 면이 5번 나온 결과이다. $K$개의 class가 있다면 $D$개 면을 가진 주사위 $K$개가 있다고 보고, 주사위를 던진 결과로부터 $1, … ,K$중 어떤 주사위를 던졌는지 찾아내는 것이라고 이해한다. <u>문서 내에 특정 단어가 몇번 등장하는지에 대한 횟수를 모형화 할 수 있다.</u></p>

<p>베르누이분포는 $x$의 원소가 0 또는 1 값만을 가질 수 있다. 위와 다르게 독립변수는 $D$개의 독립적인 확률변수를 가지고 있는, 동전으로 구성된 동전 세트로 표현할 수 있다. 각각의 값은 0 또는 1이다. $K$개의 클래스를 가지고 있다고 할 때, 전체 $D * K$의 조합의 동전이 존재하며 같은 class에 속하는 D개의 동전이 하나의 동전 세트를 구성하고 이런 동전 세트가 $K$개 있다고 볼 수 있다. 즉 베르누이를 사용한 나이브 베이즈 모형은 동전 세트를 N번 던진 결과로부터 1, …, $K$ 중 어느 동전 세트를 던졌는지 찾아내는 것이다. <u>문서 내에 특정한 단어가 포함되어 있는지의 여부로 확률을 판단할 때 주로 사용한다.</u></p>

<ul>
  <li>feature_count: 각 class k에 대해 d번째 동전이 앞면이 나온 횟수 $N_d,_k$</li>
  <li>feature_log_prob: 베르누이분포 모수의 로그값</li>
</ul>

<script type="math/tex; mode=display">log\mu_k = (log\mu_1,_k,...,log\mu_D,_k) = (log\frac{N_1,_k}{N_k},...,log\frac{N_D,_k}{N_k})</script>

<p>$N_k$는 class k에 대해서 동전을 던진 횟수이다.</p>

<hr />

<p>다음 파이썬 코드를 잠깐 훑으며 베르누이 확률분포를 사용해 나이브베이즈 분류 확률을 구하는 과적을 살펴보자.</p>

<pre><code class="language-pseudocode">x = np.array([
[0, 1, 1, 0],
[1, 1, 1, 1],
[1, 1, 1, 0],
[0, 1, 0, 0],
[0, 0, 0, 1],
[0, 1, 1, 0],
[0, 1, 1, 1],
[1, 0, 1, 0],
[1, 0, 1, 1],
[0, 1, 1, 0]])

y = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1]) //class 분류
</code></pre>

<p>클래스는 0과 1로 총 2개라고 본다. $x$는 확률변수를 가지고 있는 동전 세트로 볼 수 있고, $y$는 각 세트에 대한 클래스를 정의해 놓은 것이다. 처음 4개 세트는 class 0, 다음 6개 세트는 class 1이다.</p>

<p>각 클래스 $k$별, 독립변수 $d$별로 총 8개의 베르누이 확률변수의 모수를 구하면 다음과 같다. (각 클래스 별로 합치는 것)</p>

<pre><code class="language-pseudocode">array([[2, 4, 3, 1],
	   [2, 3, 5, 3]])
</code></pre>

<p>$[2, 4, 3, 1]$이 어떻게 나왔는지 간단하게 설명해보겠다. $x$의 첫번 째 4 세트가 class 0이므로 각각 첫번째 요소가 1인 횟수를 더하여서 2, 두번째 요소가 1인 횟수를 더하여서 4, … 이렇게 합친다.</p>

<p>이렇게 합쳐진 요소들에 대해서 각 클래스의 전체 개수로 나누어 주어야 한다. 위 예시의 경우 class 0의 전체 개수는 4이고 1의 전체 개수는 6이다. 결과는 다음과 같다.</p>

<pre><code class="language-pseudocode">array([[0.5,   1,   0.75,    0.25],
	   [0.333, 0.5, 0.83333, 0.5]])
</code></pre>

<p>여기서 이제 <strong>스무딩(Smoothing)</strong>을 해야한다. 표본 데이터의 수가 그렇지 않음에도 불구하고 0 또는 1이라는 극단적인 값이 나오게 된다. <small>(현실에서 그런 확률은 거의 없다)</small> 따라서 이런 현상을 방지하기 위해서 베르누이는 모수가 0.5인 가장 일반적인 경를 가정하여서 0이 나오는 경우와 1이 나오는 경우의 가장 표본 데이터를 추가하여 스무딩한다. 주로 smoothing은 가중치 $\alpha$의 값으로 스무딩을 조절한다. 이것을 <em>==라플라스 스무딩(Laplace smooting)==</em> 또는 <em>==애드원(Add-One) 스무딩==</em>이라고 한다.</p>

<script type="math/tex; mode=display">\mu_d,_k = \frac {N_d,_k + \alpha} {N_k + 2\alpha}</script>

<p>위에서 스무딩 가중치 $\alpha$를 1.0을 주었을 때 결과 값은 다음과 같다. 확인해보면 1과 같은 극단적인 값이 없어졌음을 볼 수 있다.</p>

<pre><code class="language-pseudocode">array([[0.5,   0.833333, 0.66667, 0.33333],
	   [0.375, 0.5,      0.75,    0.5]])
</code></pre>

<p>예측을 하기 위해 $[0, 0, 1, 1]$ 을 입력하면 $array([0.0953 , 0.9046])$ 값이 나온다. 즉 3, 4번 키워드가 포함되어 있다면 class 1 일 확률이 90%라는 의미이다.</p>

<h3 id="implementation---문서-분류기-구현">Implementation - 문서 분류기 구현</h3>

<p>원리를 이해했다면 구현은 생각보다 간단하다. 물론 파이썬의 sklearn등의 모듈을 사용하면 나이브 베이즈와 각 확률분포에 대한 기능이 모두 구현되어 있다. 따라서 가져가 쓰기면 하면 된다. 여기서는 문서분류기에 해당 알고리즘을 모듈을 사용하지 않고 어떻게 구현해야 하는지를 다룰 것이다. 앞서 말했든 사내에서 쓰는 언어를 사용한 것이 때문에 코드구현은 플로우만 참고하는 것을 추천한다.</p>

<h4 id="학습-learn">학습 Learn</h4>

<ul>
  <li>Input: 텍스트, 분류 클래스</li>
</ul>

<ol>
  <li>문서 키워드를 추출한다. 키워드 추출은 사용자마다 다른 기능이나 모듈을 가져와서 기준에 따라 추출할 수 있다.</li>
  <li>중복 제거를 위해 추출된 키워드를 Set에 입력시킨다.</li>
  <li>클래스 횟수가 저장되어 있는 자료구조에 해당 클래수 횟수를 1 증가시킨다.</li>
  <li>해당 클래스의 해당 단어의 여부를 기록하기 위해 해당 자료구조에 1을 더한다.</li>
  <li>전체 단어를 저장하는 자료구조에 단어를 추가한다.</li>
</ol>

<p>위에서 설명한 베르누이 확률분포를 계산하기 위한 $N_d,_k$와 $N_k$를 계산하는 과정으로 이해하면된다. 다음은 해당을 특정 언어로 코딩한 일부분이다. 여기서 m_n_cls와 m_n_cls_word등의 자료구조는 hash이고 m_words는 set이다.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span> <span class="n">BernoulliModel</span><span class="o">::</span><span class="n">learn</span><span class="p">(</span><span class="n">string</span> <span class="n">text</span><span class="p">,</span> <span class="n">string</span> <span class="n">cls</span><span class="p">){</span>
    <span class="n">list</span><span class="o">&lt;</span><span class="n">string</span><span class="o">&gt;</span> <span class="n">tok</span><span class="p">;</span>
    <span class="n">tok</span> <span class="o">=</span> <span class="n">extract_words</span><span class="p">(</span><span class="n">text</span><span class="p">.</span><span class="n">trim</span><span class="p">(),</span> <span class="n">m_lang</span><span class="p">,</span> <span class="n">m_charset</span><span class="p">;</span>
    <span class="n">m_n_cls</span><span class="p">[</span><span class="n">cls</span><span class="p">]</span> <span class="o">+=</span><span class="mi">1</span><span class="p">;</span>
                        
    <span class="n">set</span><span class="o">&lt;</span><span class="n">string</span><span class="o">&gt;</span> <span class="n">word_set</span><span class="p">;</span>
    <span class="n">string</span> <span class="n">word</span><span class="p">;</span>
    <span class="k">for</span> <span class="n">word</span> <span class="n">in</span> <span class="n">tok</span> <span class="p">{</span>
        <span class="n">word_set</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">word</span><span class="p">);</span>
    <span class="p">}</span>                    
    
    <span class="k">for</span> <span class="n">word</span> <span class="n">in</span> <span class="n">word_set</span> <span class="p">{</span>
        <span class="n">m_n_cls_word</span><span class="p">[</span><span class="n">cls</span><span class="o">+</span><span class="s">"_"</span><span class="o">+</span><span class="n">word</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span><span class="p">;</span>
        <span class="n">m_words</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">word</span><span class="p">);</span>
    <span class="p">}</span>                    
<span class="p">}</span>
</code></pre></div></div>

<h4 id="예측-predict">예측 Predict</h4>

<ul>
  <li>Input: 예측할 테스트, Smoothing을 위한 $\alpha$</li>
</ul>

<ol>
  <li>입력 텍스트에 대해서 키워드를 추출함</li>
  <li>클래쓰 목록을 가져와서 각 클래스마다 다음을 반복함(해당 클래스의 Score를 구함)</li>
  <li>분모에 총 클래스 횟수와 smoothing값을 더함.</li>
  <li>추출된 각 키워드에 대해서 확률분포값과 smoothing 값을 더하여서 위의 분모로 나눈 log를 계산함(위에 베르누이 확률 계산 공식을 참고)</li>
  <li>각 클래스의 점수를 누적하여 예측함.</li>
</ol>

<pre><code class="language-pseudocode">BernoulliModel::predict(string fe, string lang, string charset, hash&lt;string,double&gt;&amp; score_hash, double alpha){
	list&lt;string&gt; tok;
	list&lt;string&gt; cls_list;
	double denom;
	double score;
	string cls;
	string t;
	
	tok = extract_word(fe.trim(), lang, charset);
	cls_list = m_n_cls.key();
	
	for cls in cls_list {
		socre = 0.0;
		denom = double(m_n_cls[cls]) + 2*alpha;
		for t in tok{
			score += log(m_n_cls_wor[cls+"_"+t]+alpha) / denom);
		}
		score_hash[cls] += score;
	}
}
</code></pre>

<p>다음을 예측해서 Score가 가장 높은 cls 소속임을 예측한다. 다항분포 모델이랑 비교하여 새로 구현한 베르누이 나이브 베이즈 문서 분류기의 성능을 테스트 해 보았을 때 81~82% 정도의 정확성을 보이는 것을 확인했다. 다항분포는 84~85%정도의 성능이었던 것을 고려해보면 확실히 정확한 횟수보다 여부만을 가지고 계산하는 베르누이 분류기의 성능이 다소 떨어지는 것을 확인할 수 있었다.</p>

<ul>
  <li>결과:</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/63405904/110242446-88655c00-7f99-11eb-9da7-5484473f3160.png" alt="Bernoulli" /></p>

<p><small><strong>[참고 자료]:</strong>  https://nbviewer.jupyter.org/github/metamath1/ml-simple-works/blob/master/naive/naive.ipynb, https://wikidocs.net/22892, https://heung-bae-lee.github.io/2020/04/14/machine_learning_07/</small></p>


	  ]]></description>
	</item>


</channel>
</rss>
